{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import BooleanType, StringType, FloatType, MapType, IntegerType, StructType, StructField, ArrayType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, when, max, min, lit, explode, split, regexp_replace, lower, concat, mean\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql.functions import min, max\n",
    "#from unidecode import unidecode\n",
    "import string\n",
    "import nltk\n",
    "import datetime\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = spark.read.format('csv').option(\"escape\",\"\\\"\").option('header',True).load(\"s3://msbx5420-2020/team_grays_peak/news.csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|             authors|               title|       publish_date|         description|                text|                 url|\n",
      "+---+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "|  0|        ['Cbc News']|Coronavirus a 'wa...|2020-03-27 08:00:00|Canadian pharmaci...|Canadian pharmaci...|https://www.cbc.c...|\n",
      "|  1|        ['Cbc News']|Yukon gov't names...|2020-03-27 01:45:00|The Yukon governm...|The Yukon governm...|https://www.cbc.c...|\n",
      "|  2|['The Associated ...|U.S. Senate passe...|2020-03-26 05:13:00|The Senate has pa...|The Senate late W...|https://www.cbc.c...|\n",
      "+---+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3566"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    if not text:\n",
    "        return None\n",
    "        return unidecode(str(text)).replace(\"*\", \".\").replace(\"\\n\", '.')\n",
    "\n",
    "udf_remove_non_ascii = udf(remove_non_ascii, StringType())\n",
    "\n",
    "def sent_tokenize(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    try:\n",
    "        sent_text = nltk.sent_tokenize(text)\n",
    "    except:\n",
    "        import nltk\n",
    "        nltk.download('punkt')\n",
    "        sent_text = nltk.sent_tokenize(text)\n",
    "\n",
    "    return sent_text\n",
    "\n",
    "udf_sent_tokenize = udf(sent_tokenize, ArrayType(StringType()))\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    table = str.maketrans({key: ' ' for key in string.punctuation if key!='+' and key!='-' and key!='#'})\n",
    "    text = text.translate(table)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "udf_remove_punctuations = udf(remove_punctuations, StringType())\n",
    "\n",
    "def merge_hyphen(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    return text.replace(\"-\", \"\")\n",
    "\n",
    "udf_merge_hyphen = udf(merge_hyphen, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords_1(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    from nltk import word_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    query = text\n",
    "    stop = [' i ', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "            \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', \n",
    "            'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
    "            'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "            'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', \n",
    "            'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', \n",
    "            'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', \n",
    "            'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', \n",
    "            'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', \n",
    "            'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', \n",
    "            'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', \n",
    "            'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", \n",
    "            'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", \n",
    "            'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\", 'says', ' s', 'said', 'new', 'confirmed', 'say']\n",
    "    querywords = query.split()\n",
    "    resultwords = [word for word in querywords if word.lower() not in stop]\n",
    "    result = ' '.join(resultwords)\n",
    "    return result\n",
    "    \n",
    "udf_rem_stopwords_1 = udf(remove_stopwords_1, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3566"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_1 = news.withColumn(\"description_rem_stopwords\", udf_rem_stopwords_1(news['description'])).distinct()\n",
    "news_2 = news_1.withColumn(\"news_rem_punct\", udf_remove_punctuations(news_1['description_rem_stopwords']))\n",
    "news_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------------+--------------------+\n",
      "| _c0|            authors|               title|       publish_date|         description|                text|                 url|description_rem_stopwords|      news_rem_punct|\n",
      "+----+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------------+--------------------+\n",
      "|1345|                 []|Health officials ...|2020-01-26 15:20:00|Health officials ...|Federal health of...|https://www.cbc.c...|     Health officials ...|Health officials ...|\n",
      "|1364|['Thomson Reuters']|Coronavirus death...|2020-01-30 00:11:00|China's National ...|China's National ...|https://www.cbc.c...|     China's National ...|China s National ...|\n",
      "|1476|                 []|Ottawa-Gatineau c...|2020-03-26 13:23:00|All city sports f...|All Ottawa- and G...|https://www.cbc.c...|     city sports field...|city sports field...|\n",
      "+----+-------------------+--------------------+-------------------+--------------------+--------------------+--------------------+-------------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_2. show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = news_2.select('news_rem_punct').rdd.flatMap(lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_rdd1 = rdd1.flatMap(lambda line: line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8642"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df = count_rdd1.toDF(['word', 'count'])\n",
    "count_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_df_filter = count_df.orderBy(desc(\"count\")).filter(\"count > 200\")\n",
    "count_df_filter.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|   COVID-19| 1451|\n",
      "|          s| 1025|\n",
      "|coronavirus| 1004|\n",
      "|     health|  511|\n",
      "|   pandemic|  489|\n",
      "|     people|  433|\n",
      "|   outbreak|  423|\n",
      "|      cases|  421|\n",
      "|     spread|  358|\n",
      "|   province|  337|\n",
      "|     Canada|  329|\n",
      "| government|  301|\n",
      "|     Health|  284|\n",
      "|     public|  262|\n",
      "|       home|  245|\n",
      "|      China|  240|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_df_filter.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
